{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"k1o8fUzEcg8x"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"id":"k1o8fUzEcg8x"},{"cell_type":"code","execution_count":null,"metadata":{"id":"dd68f505"},"outputs":[],"source":["#imports\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import numpy\n","import random\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from collections import namedtuple\n","import math\n","import argparse\n","%pip install -U gym>=0.21.0\n","%pip install -U gym[atari,accept-rom-license]\n","import gym\n","from IPython import display\n","from gym import wrappers"],"id":"dd68f505"},{"cell_type":"code","execution_count":null,"metadata":{"id":"3f06b0a7"},"outputs":[],"source":["#config.py\n","\n","Pong = {\n","    'obs_stack_size': 4,\n","    'memory_size': 50000,\n","    'n_episodes': 10000,  \n","    'batch_size': 32,\n","    'target_update_frequency': 1000,\n","    'train_frequency': 4,\n","    'gamma': 0.99,\n","    'lr': 1e-4,\n","    'eps_start': 1.0,\n","    'eps_end': 0.01,\n","    'anneal_length': 10**6,\n","    'n_actions': 2,  # 2 if we do action mapping, otherwise 6\n","}"],"id":"3f06b0a7"},{"cell_type":"code","execution_count":null,"metadata":{"id":"28809ec7"},"outputs":[],"source":["#util\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","def preprocess(obs, env):\n","    \"\"\"Performs necessary observation preprocessing.\"\"\"\n","    if env in ['Pong-v0']:\n","        return torch.tensor(obs, device=device).float()\n","    else:\n","        raise ValueError('Please add necessary observation preprocessing instructions to preprocess() in utils.py.')"],"id":"28809ec7"},{"cell_type":"code","execution_count":null,"metadata":{"id":"fe01e4d5"},"outputs":[],"source":["#dqn\n","\n","#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","class ReplayMemory:\n","    def __init__(self, capacity):\n","        self.capacity = capacity\n","        self.memory = []\n","        self.position = 0\n","\n","    def __len__(self):\n","        return len(self.memory)\n","\n","    def push(self, obs, action, next_obs, reward):\n","        if len(self.memory) < self.capacity:\n","            self.memory.append(None)\n","\n","        self.memory[self.position] = (obs, action, next_obs, reward)\n","        self.position = (self.position + 1) % self.capacity\n","\n","    def sample(self, batch_size):\n","        \"\"\"\n","        Samples batch_size transitions from the replay memory and returns a tuple\n","            (obs, action, next_obs, reward)\n","        \"\"\"\n","        sample = random.sample(self.memory, batch_size)\n","        return tuple(zip(*sample))\n","\n","\n","class DQN(nn.Module):\n","    def __init__(self, env_config):\n","        super(DQN, self).__init__()\n","\n","        # Save hyperparameters needed in the DQN class.\n","        self.batch_size = env_config[\"batch_size\"]\n","        self.gamma = env_config[\"gamma\"]\n","        self.eps_start = env_config[\"eps_start\"]\n","        self.eps_threshold = self.eps_start\n","        self.eps_end = env_config[\"eps_end\"]\n","        self.anneal_length = env_config[\"anneal_length\"]\n","        self.n_actions = env_config[\"n_actions\"]\n","\n","        self.total_episodes = env_config[\"n_episodes\"]\n","        self.episodes_done = 0\n","\n","        self.conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4, padding=0)\n","        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0)\n","        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0)\n","        self.fc1 = nn.Linear(3136, 512)\n","        self.fc2 = nn.Linear(512, self.n_actions)\n","\n","        self.relu = nn.ReLU()\n","        self.flatten = nn.Flatten()\n","\n","    def forward(self, x):\n","        \"\"\"Runs the forward pass of the NN depending on architecture.\"\"\"\n","        x = self.relu(self.conv1(x))\n","        x = self.relu(self.conv2(x))\n","        x = self.relu(self.conv3(x))\n","        x = self.flatten(x)\n","        x = self.fc1(x)\n","        x = self.fc2(x)\n","        #print(\"Shape after forward: \", x.shape)\n","        return x\n","\n","    def act(self, observation, exploit=False):\n","        \"\"\"Selects an action with an epsilon-greedy exploration strategy.\"\"\"\n","        # TODO: Implement action selection using the Deep Q-network. This function\n","        #       takes an observation tensor and should return a tensor of actions.\n","        #       For example, if the state dimension is 4 and the batch size is 32,\n","        #       the input would be a [32, 4] tensor and the output a [32, 1] tensor.\n","        # TODO: Implement epsilon-greedy exploration.\n","\n","        # Approx. 1300 steps/episode (could also measure self.episodes)\n","        #eps_threshold = self.eps_end + (self.eps_start - self.eps_end) * math.exp(-1. * self.steps_done / 500000)\n","        #print(eps_threshold)\n","        #self.steps_done += 1\n","        self.eps_start = self.eps_start * 0.9999999\n","        if self.eps_start < self.eps_end:\n","          self.eps_start = max(self.eps_start, self.eps_end)\n","        if random.random() < self.eps_start and not exploit:\n","            return torch.tensor(random.randint(2, 3)).unsqueeze(0).to(device)\n","        else:\n","            q_values = self.forward(observation)\n","            #print(\"q_values: \", q_values)\n","            action = torch.argmax(q_values,1)+2\n","            #print(\"computed action: \", action)\n","            return action\n","\n","def optimize(dqn, target_dqn, memory, optimizer):\n","    \"\"\"This function samples a batch from the replay buffer and optimizes the Q-network.\"\"\"\n","    # If we don't have enough transitions stored yet, we don't train.\n","    if len(memory) < dqn.batch_size:\n","        return\n","\n","    # TODO: Sample a batch from the replay memory and concatenate so that there are\n","    #       four tensors in total: observations, actions, next observations and rewards.\n","    #       Remember to move them to GPU if it is available, e.g., by using Tensor.to(device).\n","    #       Note that special care is needed for terminal transitions!\n","    transitions = memory.sample(dqn.batch_size)\n","    obs_batch = torch.cat(transitions[0]).to(device)\n","    action_batch = torch.cat(transitions[1]).to(device)\n","    non_terminal_next_obs = [s for s in transitions[2] if s.size(1) == 4]\n","    next_obs_batch = torch.cat(non_terminal_next_obs).to(device)\n","    reward_batch = torch.cat(transitions[3]).to(device)\n","\n","    # TODO: Compute the current estimates of the Q-values for each state-action\n","    #       pair (s,a). Here, torch.gather() is useful for selecting the Q-values\n","    #       corresponding to the chosen actions.\n","    all_q_values = dqn.forward(obs_batch)\n","    action_indices = (action_batch - 2).unsqueeze(1)\n","    q_values = torch.gather(all_q_values, 1, action_indices)\n","    \n","    # TODO: Compute the Q-value targets. Only do this for non-terminal transitions!\n","    \n","    all_next_state_values = torch.zeros(dqn.batch_size, dqn.n_actions, device=device)\n","    \n","    is_non_terminal = torch.tensor(tuple(map(lambda s: s.size(1) == 4,\n","                                            transitions[2])), device=device, dtype=torch.bool)\n","    \n","    all_next_state_values[is_non_terminal] = target_dqn.forward(next_obs_batch)\n","    best_next_state_values = all_next_state_values.max(1)[0]\n","    q_value_targets = dqn.gamma * best_next_state_values + reward_batch\n","    \n","    # Compute loss.\n","    loss = F.mse_loss(q_values.squeeze(), q_value_targets)\n","\n","    # Perform gradient descent.\n","    optimizer.zero_grad()\n","\n","    loss.backward()\n","    optimizer.step()\n","\n","    return loss.item()"],"id":"fe01e4d5"},{"cell_type":"code","execution_count":null,"metadata":{"id":"d23503ed"},"outputs":[],"source":["#evaluate\n","\n","\"\"\"\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","parser = argparse.ArgumentParser()\n","parser.add_argument('--env', choices=['Pong'])\n","parser.add_argument('--path', type=str, help='Path to stored DQN model.')\n","parser.add_argument('--n_eval_episodes', type=int, default=1, help='Number of evaluation episodes.', nargs='?')\n","parser.add_argument('--render', dest='render', action='store_true', help='Render the environment.')\n","parser.add_argument('--save_video', dest='save_video', action='store_true', help='Save the episodes as video.')\n","parser.set_defaults(render=False)\n","parser.set_defaults(save_video=False)\n","\n","# Hyperparameter configurations for different environments. See config.py.\n","ENV_CONFIGS = {\n","    'Pong-v0': Pong,\n","}\n","\"\"\"\n","\n","def evaluate_policy(dqn, env, env_config, args, n_episodes, render=False, verbose=False):\n","    \"\"\"Runs {n_episodes} episodes to evaluate current policy.\"\"\"\n","    \n","    total_return = 0\n","\n","    for i in range(n_episodes):\n","        obs = preprocess(env.reset(), env=args[\"env\"]).unsqueeze(0)\n","        obs_stack = torch.cat(env_config['obs_stack_size'] * [obs]).unsqueeze(0).to(device)\n","\n","        done = False\n","        episode_return = 0\n","\n","        while not done:\n","            if render:\n","                env.render()\n","\n","            action = dqn.act(obs_stack, exploit=True).item()\n","            #print(\"eval action: \", action)\n","\n","            obs, reward, done, info = env.step(action)\n","            obs = preprocess(obs, env=args[\"env\"]).unsqueeze(0)\n","            \n","            if not done:\n","                obs_stack = torch.cat((obs_stack[:, 1:, ...], obs.unsqueeze(1)), dim=1).to(device)\n","            #obs = preprocess(obs, env=args.env).unsqueeze(0)\n","\n","            episode_return += reward\n","        \n","        total_return += episode_return\n","        \n","        #if verbose:\n","            #print(f'Finished episode {i+1} with a total return of {episode_return}')\n","\n","    \n","    return total_return / n_episodes"],"id":"d23503ed"},{"cell_type":"code","execution_count":null,"metadata":{"id":"9fe56702"},"outputs":[],"source":["#train\n","\n","#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\"\"\"\n","parser = argparse.ArgumentParser()\n","parser.add_argument('--env', choices=['Pong-v0'])\n","parser.add_argument('--evaluate_freq', type=int, default=25, help='How often to run evaluation.', nargs='?')\n","parser.add_argument('--evaluation_episodes', type=int, default=5, help='Number of evaluation episodes.', nargs='?')\n","\"\"\"\n","\n","args = { \"evaluate_freq\": 25, \"evaluation_episodes\":5, \"env\":'Pong-v0' }\n","\n","# Hyperparameter configurations for different environments. See config.py.\n","ENV_CONFIGS = {\n","    'Pong-v0': Pong\n","}\n","\n","#args = parser.parse_args()\n","\n","# Initialize environment and config.\n","env = gym.make(args[\"env\"])\n","env_config = ENV_CONFIGS[args[\"env\"]]\n","\n","env  =  gym.wrappers.AtariPreprocessing (env, screen_size=84 , grayscale_obs=True , frame_skip=1 , noop_max=30, scale_obs=True)\n","\n","# Initialize deep Q-networks.\n","dqn = DQN(env_config=env_config).to(device)\n","# dqn = torch.load('drive/MyDrive/Pong/Pong-v0_best.pt', map_location=torch.device(device))\n","\n","# TODO: Create and initialize target Q-network.\n","target_dqn = DQN(env_config=env_config).to(device)\n","\n","# Create replay memory.\n","memory = ReplayMemory(env_config['memory_size'])\n","\n","# Initialize optimizer used for training the DQN. We use Adam rather than RMSProp.\n","optimizer = torch.optim.Adam(dqn.parameters(), lr=env_config['lr'])\n","\n","# Keep track of best evaluation mean return achieved so far.\n","best_mean_return = -float(\"Inf\")\n","mean_return_history = []\n","\n","for episode in range(dqn.episodes_done, dqn.total_episodes):\n","    \n","    done = False\n","    steps = 0\n","    dqn.eps_threshold = dqn.eps_end + (dqn.eps_start - dqn.eps_end) * math.exp(-10 * dqn.episodes_done / dqn.total_episodes)\n","    print(dqn.eps_threshold)\n","    \n","    obs = preprocess(env.reset(), env=args[\"env\"]).unsqueeze(0)\n","    obs_stack = torch.cat(env_config['obs_stack_size'] * [obs]).unsqueeze(0).to(device)\n","        \n","    while not done:\n","        # TODO: Get action from DQN.\n","        action = dqn.act(obs_stack)\n","        #print(\"action: \", action)\n","\n","        # Act in the true environment.\n","        obs, reward, done, info = env.step(action)\n","        steps += 1\n","\n","        # Preprocess incoming observation.\n","        old_obs_stack = obs_stack\n","        if not done:\n","            obs = preprocess(obs, env=args[\"env\"]).unsqueeze(0)\n","            obs_stack = torch.cat((obs_stack[:, 1:, ...], obs.unsqueeze(1)), dim=1).to(device)\n","        else:\n","            #obs = torch.zeros(1,84,84)\n","            #print(obs.shape)\n","            #obs_stack = torch.cat((obs_stack[:, 1:, ...], obs.unsqueeze(1)), dim=1).to(device)\n","            obs_stack = obs_stack[:, 1:, ...]\n","            #print(obs_stack.shape)\n","            \n","            \n","        # TODO: Add the transition to the replay memory. Remember to convert\n","        #       everything to PyTorch tensors!\n","        reward = torch.tensor(reward).unsqueeze(0)\n","        memory.push(old_obs_stack, action, obs_stack, reward)\n","\n","         # TODO: Run DQN.optimize() every env_config[\"train_frequency\"] steps.\n","        if steps % env_config[\"train_frequency\"] == 0:\n","            optimize(dqn, target_dqn, memory, optimizer)\n","\n","        # TODO: Update the target network every env_config[\"target_update_frequency\"] steps.\n","        if steps % env_config[\"target_update_frequency\"] == 0:\n","            target_dqn.load_state_dict(dqn.state_dict())\n","    # Evaluate the current agent.\n","    if episode % args[\"evaluate_freq\"] == 0:\n","        print(dqn.eps_start)\n","        mean_return = evaluate_policy(dqn, env, env_config, args, n_episodes=args[\"evaluation_episodes\"], verbose=True)\n","        mean_return_history.append(mean_return)\n","        f = plt.figure()\n","        plt.plot(range(0, len(mean_return_history) * args[\"evaluate_freq\"], args[\"evaluate_freq\"]), mean_return_history)\n","        plt.xlabel(\"Episode\")\n","        plt.ylabel(\"Mean return\")\n","        plt.title(\"Mean return over episodes\")\n","        f.savefig(f'/content/drive/My Drive/Reinforcement_Learning_Atari/models_Pong/{args[\"env\"]}_model_tuf_{env_config[\"target_update_frequency\"]}.png')\n","    \n","        print(f'Episode {episode}/{env_config[\"n_episodes\"]}: {mean_return}')\n","\n","        # Save current agent if it has the best performance so far.\n","        if mean_return >= best_mean_return:\n","            best_mean_return = mean_return\n","\n","            print('Best performance so far! Saving model.')\n","            torch.save(dqn, f'temp_best.pt')\n","            torch.save(dqn, f'/content/drive/My Drive/Reinforcement_Learning_Atari/models_Pong/{args[\"env\"]}_best.pt')\n","\n","    dqn.episodes_done += 1        \n","# Close environment after training is completed.\n","env.close()"],"id":"9fe56702"},{"cell_type":"code","execution_count":null,"metadata":{"id":"55857529"},"outputs":[],"source":["\n","args = { \"evaluate_freq\": 25, \"evaluation_episodes\":5, \"env\":'Pong-v0' }\n","\n","# Hyperparameter configurations for different environments. See config.py.\n","ENV_CONFIGS = {\n","    'Pong-v0': Pong\n","}\n","dqn = torch.load('drive/MyDrive/Reinforcement_Learning_Atari/models_Pong/Pong-v0_best.pt', map_location=torch.device(device))\n","env = gym.make('Pong-v0')\n","env = gym.wrappers.AtariPreprocessing ( env , screen_size = 84 , grayscale_obs = True , frame_skip = 1 , noop_max = 30,scale_obs=True )\n","env.reset()\n","env_config = ENV_CONFIGS[\"Pong-v0\"]\n","args = { \"evaluate_freq\": 25, \"evaluation_episodes\":5, \"env\":'Pong-v0' }\n","\n","\n","\n","done = False\n","obs = preprocess(env.reset(), env=args[\"env\"]).unsqueeze(0)\n","obs_stack = torch.cat(env_config['obs_stack_size'] * [obs]).unsqueeze(0).to(device)\n","while not done:\n","    frame = env.render(mode=\"rgb_array\")\n","    plt.imshow(frame)\n","    plt.show() \n","    display.clear_output(wait=True)\n","    action = dqn.act(obs_stack,exploit=True)\n","    print(action)\n","    obs, reward, done, info = env.step(action)\n","    if not done:\n","        old_obs_stack = obs_stack\n","            \n","        obs = preprocess(obs, env=args[\"env\"]).unsqueeze(0)\n","        obs_stack = torch.cat((obs_stack[:, 1:, ...], obs.unsqueeze(1)), dim=1).to(device)\n","env.close()"],"id":"55857529"},{"cell_type":"code","execution_count":null,"metadata":{"id":"746a1a38"},"outputs":[],"source":[""],"id":"746a1a38"}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Pong_group18.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.11"}},"nbformat":4,"nbformat_minor":5}