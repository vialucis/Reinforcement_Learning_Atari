{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cartpole.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Furpr7mfYmRe",
        "outputId": "dc5fe735-7334-4754-f27f-6a9eb82a50ec"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "L6P7BAWSK7J-"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy\n",
        "import gym\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from collections import namedtuple\n",
        "import math\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "In this file, you may edit the hyperparameters used for different environments.\n",
        "\n",
        "memory_size: Maximum size of the replay memory.\n",
        "n_episodes: Number of episodes to train for.\n",
        "batch_size: Batch size used for training DQN.\n",
        "target_update_frequency: How often to update the target network.\n",
        "train_frequency: How often to train the DQN.\n",
        "gamma: Discount factor.\n",
        "lr: Learning rate used for optimizer.\n",
        "eps_start: Starting value for epsilon (linear annealing).\n",
        "eps_end: Final value for epsilon (linear annealing).\n",
        "anneal_length: How many steps to anneal epsilon for.\n",
        "n_actions: The number of actions can easily be accessed with env.action_space.n, but we do\n",
        "    some manual engineering to account for the fact that Pong has duplicate actions.\n",
        "\"\"\"\n",
        "\n",
        "# Hyperparameters for CartPole-v0\n",
        "CartPole = {\n",
        "    'memory_size': 50000,\n",
        "    'n_episodes': 10000,\n",
        "    'batch_size': 32,\n",
        "    'target_update_frequency': 100,\n",
        "    'train_frequency': 1,\n",
        "    'gamma': 0.95,\n",
        "    'lr': 1e-4,\n",
        "    'eps_start': 1.0,\n",
        "    'eps_end': 0.05,\n",
        "    'anneal_length': 10**4,\n",
        "    'n_actions': 2,\n",
        "}\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def preprocess(obs, env):\n",
        "    \"\"\"Performs necessary observation preprocessing.\"\"\"\n",
        "    if env in ['CartPole-v0']:\n",
        "        return torch.tensor(obs, device=device).float()\n",
        "    else:\n",
        "        raise ValueError('Please add necessary observation preprocessing instructions to preprocess() in utils.py.')\n"
      ],
      "metadata": {
        "id": "caKt0Oo8eYZ4"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "# Store Transitions\n",
        "class ReplayMemory:\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "        self.position = 0\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "    def push(self, obs, action, next_obs, reward):\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(None)\n",
        "\n",
        "        self.memory[self.position] = (obs, action, next_obs, reward)\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"\n",
        "        Samples batch_size transitions from the replay memory and returns a tuple\n",
        "            (obs, action, next_obs, reward)\n",
        "        \"\"\"\n",
        "        sample = random.sample(self.memory, batch_size)\n",
        "        return tuple(zip(*sample))\n",
        "\n",
        "\n",
        "steps_done = 0\n",
        "\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, env_config):\n",
        "        super(DQN, self).__init__()\n",
        "\n",
        "        # Save hyperparameters needed in the DQN class.\n",
        "        self.batch_size = env_config[\"batch_size\"]\n",
        "        self.gamma = env_config[\"gamma\"]\n",
        "        self.eps_start = env_config[\"eps_start\"]\n",
        "        self.eps_end = env_config[\"eps_end\"]\n",
        "        self.anneal_length = env_config[\"anneal_length\"]\n",
        "        self.n_actions = env_config[\"n_actions\"]\n",
        "        self.steps_done = 0\n",
        "\n",
        "        self.fc1 = nn.Linear(4, 256)\n",
        "        self.fc2 = nn.Linear(256, self.n_actions)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Runs the forward pass of the NN depending on architecture.\"\"\"\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def act(self, observation, exploit=False):\n",
        "        \"\"\"Selects an action with an epsilon-greedy exploration strategy.\"\"\"\n",
        "        # TODO: Implement action selection using the Deep Q-network. This function\n",
        "        #       takes an observation tensor and should return a tensor of actions.\n",
        "        #       For example, if the state dimension is 4 and the batch size is 32,\n",
        "        #       the input would be a [32, 4] tensor and the output a [32, 1] tensor.\n",
        "        # TODO: Implement epsilon-greedy exploration.\n",
        "        global steps_done\n",
        "        if exploit:\n",
        "            with torch.no_grad():\n",
        "                return self.forward(observation[0]).max(0)[1]\n",
        "        action = []\n",
        "        eps_threshold = self.eps_end + (self.eps_start - self.eps_end) * math.exp(-1. * steps_done / 200)\n",
        "        steps_done += 1\n",
        "\n",
        "        for state in observation:\n",
        "            if random.random() <= eps_threshold:\n",
        "                action_index = random.randrange(0, self.n_actions)\n",
        "                action.append(action_index)\n",
        "            else:\n",
        "                action.append(self.forward(state).max(0)[1].item())\n",
        "\n",
        "        return torch.tensor(action)\n",
        "\n",
        "\n",
        "def optimize(dqn, target_dqn, memory, optimizer):\n",
        "    \"\"\"This function samples a batch from the replay buffer and optimizes the Q-network.\"\"\"\n",
        "    # If we don't have enough transitions stored yet, we don't train.\n",
        "    if len(memory) < dqn.batch_size:\n",
        "        return\n",
        "\n",
        "    # TODO: Sample a batch from the replay memory and concatenate so that there are\n",
        "    #       four tensors in total: observations, actions, next observations and rewards.\n",
        "    #       Remember to move them to GPU if it is available, e.g., by using Tensor.to(device).\n",
        "    #       Note that special care is needed for terminal transitions!\n",
        "\n",
        "    transitions = memory.sample(dqn.batch_size)\n",
        "    Transition = namedtuple('Transition',\n",
        "                            ('obs', 'action', 'next_obs', 'reward'))\n",
        "    batch = Transition(*transitions)\n",
        "    non_final_mask = torch.tensor(tuple(map(lambda s: not isinstance(s, numpy.ndarray),\n",
        "                                            batch.next_obs)), device=device, dtype=torch.bool)\n",
        "\n",
        "    non_final_next_states = torch.cat([s for s in batch.next_obs if not isinstance(s, numpy.ndarray)])\n",
        "    state_batch = torch.cat(batch.obs).to(device)\n",
        "    action_batch = torch.cat(batch.action).to(device)\n",
        "    reward_batch = torch.cat(batch.reward).to(device)\n",
        "\n",
        "    # TODO: Compute the current estimates of the Q-values for each state-action\n",
        "    #       pair (s,a). Here, torch.gather() is useful for selecting the Q-values\n",
        "    #       corresponding to the chosen actions.\n",
        "    q_values = dqn(state_batch).gather(1, action_batch.unsqueeze(1))\n",
        "\n",
        "    # TODO: Compute the Q-value targets. Only do this for non-terminal transitions!\n",
        "\n",
        "    next_state_values = torch.zeros(dqn.batch_size, device=device)\n",
        "    next_state_values[non_final_mask] = target_dqn(non_final_next_states).detach().max(1)[0]\n",
        "\n",
        "    # Compute the expected Q values\n",
        "    q_value_targets = (next_state_values * dqn.gamma) + reward_batch\n",
        "    # Compute loss.\n",
        "    loss = F.mse_loss(q_values.squeeze(), q_value_targets)\n",
        "\n",
        "    # Perform gradient descent.\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()\n"
      ],
      "metadata": {
        "id": "6fCH48OfLBLV"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_policy(dqn, env, env_config, args, n_episodes, render=False, verbose=False):\n",
        "    \"\"\"Runs {n_episodes} episodes to evaluate current policy.\"\"\"\n",
        "    total_return = 0\n",
        "\n",
        "    for i in range(n_episodes):\n",
        "        obs = preprocess(env.reset(), env=args[\"env\"]).unsqueeze(0)\n",
        "\n",
        "        done = False\n",
        "        episode_return = 0\n",
        "\n",
        "        while not done:\n",
        "            if render:\n",
        "                env.render()\n",
        "\n",
        "            action = dqn.act(obs, exploit=True).item()\n",
        "\n",
        "            obs, reward, done, info = env.step(action)\n",
        "            obs = preprocess(obs, env=args[\"env\"]).unsqueeze(0)\n",
        "\n",
        "            episode_return += reward\n",
        "        \n",
        "        total_return += episode_return\n",
        "        \n",
        "        if verbose:\n",
        "            print(f'Finished episode {i+1} with a total return of {episode_return}')\n",
        "\n",
        "    \n",
        "    return total_return / n_episodes"
      ],
      "metadata": {
        "id": "jmQTPSLDfFdl"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "args = { \"evaluate_freq\": 25, \"evaluation_episodes\":5, \"env\":'CartPole-v0' }\n",
        "\n",
        "\n",
        "# Hyperparameter configurations for different environments. See config.py.\n",
        "ENV_CONFIGS = {\n",
        "    'CartPole-v0': CartPole\n",
        "}\n",
        "\n",
        "\n",
        "# Initialize environment and config.\n",
        "env = gym.make(args[\"env\"]).unwrapped\n",
        "env_config = ENV_CONFIGS[\"CartPole-v0\"]\n",
        "\n",
        "# Initialize deep Q-networks.\n",
        "dqn = DQN(env_config=env_config).to(device)\n",
        "target_dqn = DQN(env_config=env_config).to(device)\n",
        "target_dqn.load_state_dict(dqn.state_dict())\n",
        "target_dqn.eval()\n",
        "\n",
        "# Create replay memory.\n",
        "memory = ReplayMemory(env_config['memory_size'])\n",
        "\n",
        "# Initialize optimizer used for training the DQN. We use Adam rather than RMSProp.\n",
        "optimizer = torch.optim.Adam(dqn.parameters(), lr=env_config['lr'])\n",
        "\n",
        "# Keep track of best evaluation mean return achieved so far.\n",
        "best_mean_return = -float(\"Inf\")\n",
        "\n",
        "mean_return_history = []\n",
        "\n",
        "for episode in range(env_config['n_episodes']):\n",
        "    done = False\n",
        "\n",
        "    obs = preprocess(env.reset(), env=args[\"env\"]).unsqueeze(0)\n",
        "\n",
        "    while not done:\n",
        "\n",
        "        action = dqn.act(obs)\n",
        "\n",
        "        # Act in the true environment.\n",
        "        obs_old = obs\n",
        "        obs, reward, done, info = env.step(action.item())\n",
        "        # Preprocess incoming observation.\n",
        "        if not done:\n",
        "            obs = preprocess(obs, env=args[\"env\"]).unsqueeze(0)\n",
        "        # TODO: Add the transition to the replay memory. Remember to convert\n",
        "        #       everything to PyTorch tensors!\n",
        "        reward = torch.tensor([reward], device=device)\n",
        "        memory.push(obs_old, action, obs, reward)\n",
        "\n",
        "        # TODO: Run DQN.optimize() every env_config[\"train_frequency\"] steps.\n",
        "        if episode % env_config[\"train_frequency\"] == 0:\n",
        "            optimize(dqn, target_dqn, memory, optimizer)\n",
        "\n",
        "        # TODO: Update the target network every env_config[\"target_update_frequency\"] steps.\n",
        "        if episode % env_config[\"target_update_frequency\"] == 0:\n",
        "            target_dqn.load_state_dict(dqn.state_dict())\n",
        "\n",
        "    # Evaluate the current agent.\n",
        "    if episode % args[\"evaluate_freq\"] == 0:\n",
        "        mean_return = evaluate_policy(dqn, env, env_config, args, n_episodes=5)\n",
        "        mean_return_history.append(min(mean_return, 500))\n",
        "\n",
        "        if episode % (args[\"evaluate_freq\"]*40) == 0:\n",
        "            f = plt.figure()\n",
        "            plt.plot(range(0, len(mean_return_history) * args[\"evaluate_freq\"], args[\"evaluate_freq\"]), mean_return_history)\n",
        "            plt.axhline(y=200, color='r', linestyle='-')\n",
        "            plt.xlabel(\"Episode\")\n",
        "            plt.ylabel(\"Mean return\")\n",
        "            plt.title(\"Mean return over episodes\")\n",
        "            f.savefig(f'/content/drive/My Drive/Reinforcement_Learning_Atari/models/{args[\"env\"]}_model_tuf_{env_config[\"target_update_frequency\"]}_episode_{episode}.png')\n",
        "\n",
        "        print(f'Episode {episode}/{env_config[\"n_episodes\"]}: {mean_return}')\n",
        "\n",
        "        # Save current agent if it has the best performance so far.\n",
        "        if mean_return >= best_mean_return:\n",
        "            best_mean_return = mean_return\n",
        "\n",
        "            print('Best performance so far! Saving model.')\n",
        "            #torch.save(dqn, f'models/x_best.pt')\n",
        "            with open(f'/content/drive/My Drive/Reinforcement_Learning_Atari/models/{args[\"env\"]}_model_tuf_{env_config[\"target_update_frequency\"]}.pt', 'w') as f:\n",
        "                f.write('dqn')\n",
        "\n",
        "f = plt.figure()\n",
        "plt.plot(range(0, len(mean_return_history) * args[\"evaluate_freq\"], args[\"evaluate_freq\"]), mean_return_history)\n",
        "plt.axhline(y=200, color='r', linestyle='-')\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Mean return\")\n",
        "plt.title(\"Mean return over episodes\")\n",
        "f.savefig(f'/content/drive/My Drive/Reinforcement_Learning_Atari/models/{args[\"env\"]}_model_tuf_{env_config[\"target_update_frequency\"]}_episode_{episode}.png')\n",
        "\n",
        "# Close environment after training is completed.\n",
        "env.close()\n"
      ],
      "metadata": {
        "id": "rKOcqu8ALPYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "s9unNRS7gOAR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}