{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pong_implementation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1xWFTQgPKww",
        "outputId": "d618808d-d4ff-4e46-91fd-91af754fd59d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1sEi1L2kmSMR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3044b14d-5a88-4812-fa56-2aa4ea07bca5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[accept-rom-license,atari] in /usr/local/lib/python3.7/dist-packages (0.24.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.21.6)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (0.0.6)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (4.11.3)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.3.0)\n",
            "Collecting autorom[accept-rom-license]~=0.4.2\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Collecting ale-py~=0.7.5\n",
            "  Downloading ale_py-0.7.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 33.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py~=0.7.5->gym[accept-rom-license,atari]) (5.7.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.23.0)\n",
            "Collecting AutoROM.accept-rom-license\n",
            "  Downloading AutoROM.accept-rom-license-0.4.2.tar.gz (9.8 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (4.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (3.8.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2022.5.18.1)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.4.2-py3-none-any.whl size=441027 sha256=a62e851ec92bf582d932b17cff65194987db08c4b58d575a1f1506997de629fb\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/67/2e/6147e7912fe37f5408b80d07527dab807c1d25f5c403a9538a\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: AutoROM.accept-rom-license, autorom, ale-py\n",
            "Successfully installed AutoROM.accept-rom-license-0.4.2 ale-py-0.7.5 autorom-0.4.2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py:424: UserWarning: \u001b[33mWARN: Custom namespace `ALE` is being overridden by namespace `ALE`. If you are developing a plugin you shouldn't specify a namespace in `register` calls. The namespace is specified through the entry point package metadata.\u001b[0m\n",
            "  f\"Custom namespace `{spec.namespace}` is being overridden \"\n"
          ]
        }
      ],
      "source": [
        "#imports\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import numpy\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from collections import namedtuple\n",
        "import math\n",
        "import argparse\n",
        "%pip install -U gym>=0.21.0\n",
        "%pip install -U gym[atari,accept-rom-license]\n",
        "import gym\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#config.py\n",
        "\n",
        "\"\"\"\n",
        "In this file, you may edit the hyperparameters used for different environments.\n",
        "memory_size: Maximum size of the replay memory.\n",
        "n_episodes: Number of episodes to train for.\n",
        "batch_size: Batch size used for training DQN.\n",
        "target_update_frequency: How often to update the target network.\n",
        "train_frequency: How often to train the DQN.\n",
        "gamma: Discount factor.\n",
        "lr: Learning rate used for optimizer.\n",
        "eps_start: Starting value for epsilon (linear annealing).\n",
        "eps_end: Final value for epsilon (linear annealing).\n",
        "anneal_length: How many steps to anneal epsilon for.\n",
        "n_actions: The number of actions can easily be accessed with env.action_space.n, but we do\n",
        "    some manual engineering to account for the fact that Pong has duplicate actions.\n",
        "\"\"\"\n",
        "\n",
        "# Hyperparameters for CartPole-v0\n",
        "CartPole = {\n",
        "    'memory_size': 50000,\n",
        "    'n_episodes': 10000,\n",
        "    'batch_size': 32,\n",
        "    'target_update_frequency': 100,\n",
        "    'train_frequency': 1,\n",
        "    'gamma': 0.95,\n",
        "    'lr': 1e-4,\n",
        "    'eps_start': 1.0,\n",
        "    'eps_end': 0.05,\n",
        "    'anneal_length': 10**4,\n",
        "    'n_actions': 2,\n",
        "}\n",
        "\n",
        "Pong = {\n",
        "    'obs_stack_size': 4,\n",
        "    'memory_size': 10000,\n",
        "    'n_episodes': 200,  # 10000,\n",
        "    'batch_size': 128, # 32,\n",
        "    'target_update_frequency': 5000, # 1000,\n",
        "    'train_frequency': 4,\n",
        "    'gamma': 0.99,\n",
        "    'lr': 1e-4,\n",
        "    'eps_start': 1.0,\n",
        "    'eps_end': 0.01,\n",
        "    'anneal_length': 10**6,\n",
        "    'n_actions': 2,  # 2 if we do action mapping, otherwise 6\n",
        "}"
      ],
      "metadata": {
        "id": "yfYdL3ibmZSI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#util.py\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def preprocess(obs, env):\n",
        "    \"\"\"Performs necessary observation preprocessing.\"\"\"\n",
        "    if env in ['CartPole-v0']:\n",
        "        return torch.tensor(obs, device=device).float() #.unsqueeze(0)\n",
        "    elif env in ['Pong-v0']:\n",
        "        env_config = Pong\n",
        "        obs = torch.tensor(obs, device=device).float().unsqueeze(0)\n",
        "        #initialize the frame stack\n",
        "        obs_stack = torch.cat(env_config[\"obs_stack_size\"]  * [obs]).to(device) # .unsqueeze(0)\n",
        "        return obs_stack\n",
        "    else:\n",
        "        raise ValueError('Please add necessary observation preprocessing instructions to preprocess() in utils.py.')\n",
        "\n",
        "def add_new_obs_to_stack(obs, obs_stack, env):\n",
        "    if env in ['Pong-v0']:\n",
        "        env_config = Pong\n",
        "        obs = torch.tensor(obs[None, :], device=device).float() # .unsqueeze(0)\n",
        "        #updating the frame stack\n",
        "        next_obs_stack = torch.cat((obs_stack[1:,:, ...], obs), dim=0).to(device) # .unsqueeze(1)\n",
        "        return next_obs_stack"
      ],
      "metadata": {
        "id": "_U6Zi9RCpmnN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dqn.py\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Store Transitions\n",
        "class ReplayMemory:\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "        self.position = 0\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "    def push(self, obs, action, next_obs, reward):\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(None)\n",
        "\n",
        "        self.memory[self.position] = (obs, action, next_obs, reward)\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"\n",
        "        Samples batch_size transitions from the replay memory and returns a tuple\n",
        "            (obs, action, next_obs, reward)\n",
        "        \"\"\"\n",
        "        sample = random.sample(self.memory, batch_size)\n",
        "        return tuple(zip(*sample))\n",
        "\n",
        "\n",
        "#steps_done = 0\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, env_config):\n",
        "        super(DQN, self).__init__()\n",
        "\n",
        "        # Save hyperparameters needed in the DQN class.\n",
        "        self.batch_size = env_config[\"batch_size\"]\n",
        "        self.gamma = env_config[\"gamma\"]\n",
        "        self.eps_start = env_config[\"eps_start\"]\n",
        "        self.eps_end = env_config[\"eps_end\"]\n",
        "        self.anneal_length = env_config[\"anneal_length\"]\n",
        "        self.n_actions = env_config[\"n_actions\"]\n",
        "        self.steps_done = 0\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4, padding=0)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0)\n",
        "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0)\n",
        "        self.fc1 = nn.Linear(3136, 512)\n",
        "        self.fc2 = nn.Linear(512, self.n_actions)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(4, 32, kernel_size=8, stride=4, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(3136, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 2),\n",
        "        )\n",
        "\n",
        "\n",
        "    #def forward(self,x):\n",
        "    #  return self.fc(self.cnn(x))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # input currently: [4, 84, 84]\n",
        "\n",
        "        #print(\"Start of forward: \", x.shape)\n",
        "        x = x.view(-1, 4, 84, 84)\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.relu(self.conv2(x))\n",
        "        x = self.relu(self.conv3(x))\n",
        "        x = x.view(-1, 3136)\n",
        "        # x = x.flatten()\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x) \n",
        "        x = x.view(self.n_actions,-1)\n",
        "        return x\n",
        "\n",
        "\n",
        "    def act(self, observation, exploit=False):\n",
        "        \"\"\"Selects an action with an epsilon-greedy exploration strategy.\"\"\"\n",
        "        # TODO: Implement action selection using the Deep Q-network. This function\n",
        "        #       takes an observation tensor and should return a tensor of actions.\n",
        "        #       For example, if the state dimension is 4 and the batch size is 32,\n",
        "        #       the input would be a [32, 4] tensor and the output a [32, 1] tensor.\n",
        "        # TODO: Implement epsilon-greedy exploration.\n",
        "        #global steps_done\n",
        "        action = []\n",
        "        #action = 0\n",
        "        if exploit:\n",
        "          next_action = self.forward(observation).max(0)[1]+2\n",
        "          action.append(next_action)\n",
        "          #print(\"exploit gets the action\",action)\n",
        "          return torch.tensor(action)\n",
        "\n",
        "        eps_threshold = self.eps_end + (self.eps_start - self.eps_end) * math.exp(-1. * self.steps_done / 200)\n",
        "        self.steps_done += 1\n",
        "\n",
        "        #for state in observation:\n",
        "        #    if random.random() <= eps_threshold:\n",
        "        #        action = random.randint(0, self.n_actions)\n",
        "        #        #action = random.randint(2, 3)\n",
        "        #        #action_index = random.randint(2, 3)\n",
        "        #        action.append(action)\n",
        "        #        #print(\"random action: \", action_index)\n",
        "        #    else:\n",
        "        #        # state = state[None, :]\n",
        "        #        #action = self.forward(state).max(0)[1]+2\n",
        "        #        # ac = self.forward(state).max(0)[1].item()+2\n",
        "        #        # action.append(ac)\n",
        "        #        action.append(self.forward(observation).max(0)[1])\n",
        "        #        #print(\"chosen action: \", ac)\n",
        "        #print(action)\n",
        "\n",
        "        if random.random() <= eps_threshold:\n",
        "              \n",
        "              action.append(random.randint(2,3))\n",
        "              #print(\"random action\",action)\n",
        "        else:\n",
        "              next_action = self.forward(observation).max(0)[1]+2\n",
        "              action.append(next_action)\n",
        "              #print(\"no random\",action)\n",
        "\n",
        "        return torch.tensor(action)\n",
        "\n",
        "\n",
        "def optimize(dqn, target_dqn, memory, optimizer):\n",
        "    \"\"\"This function samples a batch from the replay buffer and optimizes the Q-network.\"\"\"\n",
        "    # If we don't have enough transitions stored yet, we don't train.\n",
        "    if len(memory) < dqn.batch_size:\n",
        "        return\n",
        "\n",
        "    #print(\"optimizing\")\n",
        "\n",
        "\n",
        "    # TODO: Sample a batch from the replay memory and concatenate so that there are\n",
        "    #       four tensors in total: observations, actions, next observations and rewards.\n",
        "    #       Remember to move them to GPU if it is available, e.g., by using Tensor.to(device).\n",
        "    #       Note that special care is needed for terminal transitions!\n",
        "\n",
        "    #print(memory.memory[0])\n",
        "    #print(\"Obs shape: \", memory.memory[0][0].shape)\n",
        "    #print(\"action: \", memory.memory[0][1])\n",
        "    #print(\"Next obs shape\", memory.memory[0][2].shape)\n",
        "    transitions = memory.sample(dqn.batch_size)\n",
        "    Transition = namedtuple('Transition',\n",
        "                            ('obs', 'action', 'next_obs', 'reward'))\n",
        "    \n",
        "    batch = Transition(*transitions)\n",
        "    non_final_mask = torch.tensor(tuple(map(lambda s: not isinstance(s, numpy.ndarray),\n",
        "                                            batch.next_obs)), device=device, dtype=torch.bool)\n",
        "    \n",
        "    non_final_next_states = torch.cat([s for s in batch.next_obs if not isinstance(s, numpy.ndarray)])\n",
        "    state_batch = torch.cat(batch.obs).to(device)\n",
        "    action_batch = torch.cat(batch.action).to(device)\n",
        "    reward_batch = torch.cat(batch.reward).to(device)\n",
        "    # TODO: Compute the current estimates of the Q-values for each state-action\n",
        "    #       pair (s,a). Here, torch.gather() is useful for selecting the Q-values\n",
        "    #       corresponding to the chosen actions.\n",
        "    #print(\"state_batch shape: \", state_batch.shape)\n",
        "    q_values = dqn(state_batch).gather(1, action_batch.view(1,dqn.batch_size))\n",
        "\n",
        "\n",
        "    # TODO: Compute the Q-value targets. Only do this for non-terminal transitions!\n",
        "    next_state_values = torch.zeros(dqn.batch_size, device=device)\n",
        "    #print(target_dqn(non_final_next_states).detach().max(0)[0].shape)\n",
        "    next_state_values[non_final_mask] = target_dqn(non_final_next_states).detach().max(0)[0]\n",
        "    # Compute the expected Q values\n",
        "    q_value_targets = (next_state_values * dqn.gamma) + reward_batch\n",
        "    # Compute loss.\n",
        "    loss = F.mse_loss(q_values.squeeze(), q_value_targets)\n",
        "    # Perform gradient descent.\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()"
      ],
      "metadata": {
        "id": "8K8GkCHRmhiJ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train.py\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\"\"\"\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--env', choices=['CartPole-v0','Pong-v0'])\n",
        "parser.add_argument('--evaluate_freq', type=int, default=25, help='How often to run evaluation.', nargs='?')\n",
        "parser.add_argument('--evaluation_episodes', type=int, default=5, help='Number of evaluation episodes.', nargs='?')\n",
        "\"\"\"\n",
        "\n",
        "args = { \"evaluate_freq\": 25, \"evaluation_episodes\":5, \"env\":'Pong-v0' }\n",
        "\n",
        "# Hyperparameter configurations for different environments. See config.py.\n",
        "ENV_CONFIGS = {\n",
        "    'CartPole-v0': CartPole,\n",
        "    'Pong-v0' : Pong,\n",
        "}\n",
        "\n",
        "# Initialize environment and config.\n",
        "env = gym.make(args[\"env\"]).unwrapped\n",
        "env_config = ENV_CONFIGS[\"Pong-v0\"]\n",
        "\n",
        "#Preprocessing the environment and scaling observations to [0,1]\n",
        "env  =  gym.wrappers.AtariPreprocessing (env, screen_size=84 , grayscale_obs=True , frame_skip=1 , noop_max=30, scale_obs=True )\n",
        "#env = gym.wrappers.FrameStack(env, 4)\n",
        "\n",
        "# Initialize deep Q-networks.\n",
        "dqn = DQN(env_config=env_config).to(device)\n",
        "target_dqn = DQN(env_config=env_config).to(device)\n",
        "target_dqn.load_state_dict(dqn.state_dict())\n",
        "target_dqn.eval()\n",
        "\n",
        "#print(dqn.parameters())\n",
        "\n",
        "# Create replay memory.\n",
        "memory = ReplayMemory(env_config['memory_size'])\n",
        "\n",
        "# Initialize optimizer used for training the DQN. We use Adam rather than RMSProp.\n",
        "optimizer = torch.optim.Adam(dqn.parameters(), lr=env_config['lr'])\n",
        "\n",
        "# Keep track of best evaluation mean return achieved so far.\n",
        "best_mean_return = -float(\"Inf\")\n",
        "\n",
        "mean_return_history = []\n",
        "\n",
        "for episode in range(env_config['n_episodes']):\n",
        "    print(\"Episode \",episode)\n",
        "    steps = 0\n",
        "    done = False\n",
        "    #obs = preprocess(env.reset(), env=args[\"env\"])\n",
        "    obs_stack = preprocess(env.reset(), env=args[\"env\"])\n",
        "    #obs_stack = obs_stack[None, :]\n",
        "    #print(\"obs_stack: \", obs_stack.shape)\n",
        "\n",
        "    steps = 0\n",
        "    while not done:\n",
        "        steps += 1\n",
        "        #print(\"step: \", steps)\n",
        "        action = dqn.act(obs_stack)\n",
        "        #print(\"action: \", action)\n",
        "\n",
        "        # Act in the true environment.\n",
        "        # obs_old = obs\n",
        "        obs_stack_old = obs_stack\n",
        "        \n",
        "        #obs, reward, done, info = env.step(action.item())\n",
        "        obs, reward, done, info = env.step(action.item())\n",
        "        if (reward > 0):\n",
        "          print(\"reward: \", reward)\n",
        "        # Preprocess incoming observation.\n",
        "        if not done:\n",
        "            #obs = obs[None, :]\n",
        "            #print(\"obs: \", obs.shape)\n",
        "            obs_stack = add_new_obs_to_stack(obs, obs_stack, env=args[\"env\"])\n",
        "        # TODO: Add the transition to the replay memory. Remember to convert\n",
        "        #       everything to PyTorch tensors!\n",
        "        reward = torch.tensor([reward], device=device)\n",
        "        memory.push(obs_stack_old, action, obs_stack, reward)\n",
        "\n",
        "        # TODO: Run DQN.optimize() every env_config[\"train_frequency\"] steps.\n",
        "        if steps % env_config[\"train_frequency\"] == 0:\n",
        "            optimize(dqn, target_dqn, memory, optimizer)\n",
        "\n",
        "        # TODO: Update the target network every env_config[\"target_update_frequency\"] steps.\n",
        "        if steps % env_config[\"target_update_frequency\"] == 0:\n",
        "            target_dqn.load_state_dict(dqn.state_dict())\n",
        "    \n",
        "    # Evaluate the current agent.\n",
        "    if episode % args[\"evaluate_freq\"] == 0:\n",
        "        print(\"saving before eval...............................................\")\n",
        "        torch.save(dqn, f'{args[\"env\"]}_preval.pt')\n",
        "        mean_return = evaluate_policy(dqn, env, env_config, args, n_episodes=args[\"evaluation_episodes\"],verbose = True)\n",
        "        mean_return_history.append(mean_return)\n",
        "\n",
        "        \"\"\"if episode % args[\"evaluate_freq\"] == 0:\n",
        "            f = plt.figure()\n",
        "            plt.plot(range(0, len(mean_return_history) * args[\"evaluate_freq\"], args[\"evaluate_freq\"]), mean_return_history)\n",
        "            plt.xlabel(\"Episode\")\n",
        "            plt.ylabel(\"Mean return\")\n",
        "            plt.title(\"Mean return over episodes\")\n",
        "            f.savefig(f'/content/drive/My Drive/Reinforcement_Learning_Atari/models_Pong/{args[\"env\"]}_model_tuf_{env_config[\"target_update_frequency\"]}_episode_{episode}.png')\"\"\"\n",
        "\n",
        "        print(f'Episode {episode}/{env_config[\"n_episodes\"]}: {mean_return}')\n",
        "\n",
        "        # Save current agent if it has the best performance so far.\n",
        "        if mean_return >= best_mean_return:\n",
        "            best_mean_return = mean_return\n",
        "\n",
        "            print('Best performance so far! Saving model.')\n",
        "            torch.save(dqn, f'{args[\"env\"]}_best.pt')\n",
        "\n",
        "# Close environment after training is completed.\n",
        "env.close()"
      ],
      "metadata": {
        "id": "P7MgQirNn7xM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9fbf979-26fe-48dd-d17c-a43b307c743a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py:569: UserWarning: \u001b[33mWARN: The environment Pong-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
            "  f\"The environment {id} is out of date. You should consider \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode  0\n",
            "saving before eval...............................................\n",
            "This is episode number 0\n",
            "Evaluated episode 1 with a total return of -21.0\n",
            "This is episode number 1\n",
            "Evaluated episode 2 with a total return of -21.0\n",
            "This is episode number 2\n",
            "Evaluated episode 3 with a total return of -21.0\n",
            "This is episode number 3\n",
            "Evaluated episode 4 with a total return of -21.0\n",
            "This is episode number 4\n",
            "Evaluated episode 5 with a total return of -21.0\n",
            "Episode 0/200: -21.0\n",
            "Best performance so far! Saving model.\n",
            "Episode  1\n",
            "Episode  2\n",
            "Episode  3\n",
            "Episode  4\n",
            "Episode  5\n",
            "Episode  6\n",
            "Episode  7\n",
            "Episode  8\n",
            "Episode  9\n",
            "Episode  10\n",
            "Episode  11\n",
            "Episode  12\n",
            "Episode  13\n",
            "Episode  14\n",
            "Episode  15\n",
            "Episode  16\n",
            "Episode  17\n",
            "Episode  18\n",
            "Episode  19\n",
            "Episode  20\n",
            "Episode  21\n",
            "Episode  22\n",
            "Episode  23\n",
            "Episode  24\n",
            "Episode  25\n",
            "saving before eval...............................................\n",
            "This is episode number 0\n",
            "Evaluated episode 1 with a total return of -21.0\n",
            "This is episode number 1\n",
            "Evaluated episode 2 with a total return of -21.0\n",
            "This is episode number 2\n",
            "Evaluated episode 3 with a total return of -21.0\n",
            "This is episode number 3\n",
            "Evaluated episode 4 with a total return of -21.0\n",
            "This is episode number 4\n",
            "Evaluated episode 5 with a total return of -21.0\n",
            "Episode 25/200: -21.0\n",
            "Best performance so far! Saving model.\n",
            "Episode  26\n",
            "Episode  27\n",
            "Episode  28\n",
            "Episode  29\n",
            "Episode  30\n",
            "Episode  31\n",
            "Episode  32\n",
            "Episode  33\n",
            "Episode  34\n",
            "Episode  35\n",
            "Episode  36\n",
            "Episode  37\n",
            "Episode  38\n",
            "Episode  39\n",
            "Episode  40\n",
            "Episode  41\n",
            "Episode  42\n",
            "Episode  43\n",
            "Episode  44\n",
            "Episode  45\n",
            "Episode  46\n",
            "Episode  47\n",
            "Episode  48\n",
            "Episode  49\n",
            "Episode  50\n",
            "saving before eval...............................................\n",
            "This is episode number 0\n",
            "Evaluated episode 1 with a total return of -21.0\n",
            "This is episode number 1\n",
            "Evaluated episode 2 with a total return of -21.0\n",
            "This is episode number 2\n",
            "Evaluated episode 3 with a total return of -21.0\n",
            "This is episode number 3\n",
            "Evaluated episode 4 with a total return of -21.0\n",
            "This is episode number 4\n",
            "Evaluated episode 5 with a total return of -21.0\n",
            "Episode 50/200: -21.0\n",
            "Best performance so far! Saving model.\n",
            "Episode  51\n",
            "Episode  52\n",
            "Episode  53\n",
            "Episode  54\n",
            "Episode  55\n",
            "Episode  56\n",
            "Episode  57\n",
            "Episode  58\n",
            "Episode  59\n",
            "Episode  60\n",
            "Episode  61\n",
            "Episode  62\n",
            "Episode  63\n",
            "Episode  64\n",
            "Episode  65\n",
            "Episode  66\n",
            "Episode  67\n",
            "Episode  68\n",
            "Episode  69\n",
            "Episode  70\n",
            "Episode  71\n",
            "Episode  72\n",
            "Episode  73\n",
            "Episode  74\n",
            "Episode  75\n",
            "saving before eval...............................................\n",
            "This is episode number 0\n",
            "Evaluated episode 1 with a total return of -21.0\n",
            "This is episode number 1\n",
            "Evaluated episode 2 with a total return of -21.0\n",
            "This is episode number 2\n",
            "Evaluated episode 3 with a total return of -21.0\n",
            "This is episode number 3\n",
            "Evaluated episode 4 with a total return of -21.0\n",
            "This is episode number 4\n",
            "Evaluated episode 5 with a total return of -21.0\n",
            "Episode 75/200: -21.0\n",
            "Best performance so far! Saving model.\n",
            "Episode  76\n",
            "Episode  77\n",
            "Episode  78\n",
            "Episode  79\n",
            "Episode  80\n",
            "Episode  81\n",
            "Episode  82\n",
            "Episode  83\n",
            "Episode  84\n",
            "Episode  85\n",
            "Episode  86\n",
            "Episode  87\n",
            "Episode  88\n",
            "Episode  89\n",
            "Episode  90\n",
            "Episode  91\n",
            "Episode  92\n",
            "Episode  93\n",
            "Episode  94\n",
            "Episode  95\n",
            "Episode  96\n",
            "Episode  97\n",
            "Episode  98\n",
            "Episode  99\n",
            "Episode  100\n",
            "saving before eval...............................................\n",
            "This is episode number 0\n",
            "Evaluated episode 1 with a total return of -21.0\n",
            "This is episode number 1\n",
            "Evaluated episode 2 with a total return of -21.0\n",
            "This is episode number 2\n",
            "Evaluated episode 3 with a total return of -21.0\n",
            "This is episode number 3\n",
            "Evaluated episode 4 with a total return of -21.0\n",
            "This is episode number 4\n",
            "Evaluated episode 5 with a total return of -21.0\n",
            "Episode 100/200: -21.0\n",
            "Best performance so far! Saving model.\n",
            "Episode  101\n",
            "Episode  102\n",
            "Episode  103\n",
            "Episode  104\n",
            "Episode  105\n",
            "Episode  106\n",
            "Episode  107\n",
            "Episode  108\n",
            "Episode  109\n",
            "Episode  110\n",
            "Episode  111\n",
            "Episode  112\n",
            "Episode  113\n",
            "Episode  114\n",
            "Episode  115\n",
            "Episode  116\n",
            "Episode  117\n",
            "Episode  118\n",
            "Episode  119\n",
            "Episode  120\n",
            "Episode  121\n",
            "Episode  122\n",
            "Episode  123\n",
            "Episode  124\n",
            "Episode  125\n",
            "saving before eval...............................................\n",
            "This is episode number 0\n",
            "Evaluated episode 1 with a total return of -21.0\n",
            "This is episode number 1\n",
            "Evaluated episode 2 with a total return of -21.0\n",
            "This is episode number 2\n",
            "Evaluated episode 3 with a total return of -21.0\n",
            "This is episode number 3\n",
            "Evaluated episode 4 with a total return of -21.0\n",
            "This is episode number 4\n",
            "Evaluated episode 5 with a total return of -21.0\n",
            "Episode 125/200: -21.0\n",
            "Best performance so far! Saving model.\n",
            "Episode  126\n",
            "Episode  127\n",
            "Episode  128\n",
            "Episode  129\n",
            "Episode  130\n",
            "Episode  131\n",
            "Episode  132\n",
            "Episode  133\n",
            "Episode  134\n",
            "Episode  135\n",
            "Episode  136\n",
            "Episode  137\n",
            "Episode  138\n",
            "Episode  139\n",
            "Episode  140\n",
            "Episode  141\n",
            "Episode  142\n",
            "Episode  143\n",
            "Episode  144\n",
            "Episode  145\n",
            "Episode  146\n",
            "Episode  147\n",
            "Episode  148\n",
            "Episode  149\n",
            "Episode  150\n",
            "saving before eval...............................................\n",
            "This is episode number 0\n",
            "Evaluated episode 1 with a total return of -21.0\n",
            "This is episode number 1\n",
            "Evaluated episode 2 with a total return of -21.0\n",
            "This is episode number 2\n",
            "Evaluated episode 3 with a total return of -21.0\n",
            "This is episode number 3\n",
            "Evaluated episode 4 with a total return of -21.0\n",
            "This is episode number 4\n",
            "Evaluated episode 5 with a total return of -21.0\n",
            "Episode 150/200: -21.0\n",
            "Best performance so far! Saving model.\n",
            "Episode  151\n",
            "Episode  152\n",
            "Episode  153\n",
            "Episode  154\n",
            "Episode  155\n",
            "Episode  156\n",
            "Episode  157\n",
            "Episode  158\n",
            "Episode  159\n",
            "Episode  160\n",
            "Episode  161\n",
            "Episode  162\n",
            "Episode  163\n",
            "Episode  164\n",
            "Episode  165\n",
            "Episode  166\n",
            "Episode  167\n",
            "Episode  168\n",
            "Episode  169\n",
            "Episode  170\n",
            "Episode  171\n",
            "Episode  172\n",
            "Episode  173\n",
            "Episode  174\n",
            "Episode  175\n",
            "saving before eval...............................................\n",
            "This is episode number 0\n",
            "Evaluated episode 1 with a total return of -21.0\n",
            "This is episode number 1\n",
            "Evaluated episode 2 with a total return of -21.0\n",
            "This is episode number 2\n",
            "Evaluated episode 3 with a total return of -21.0\n",
            "This is episode number 3\n",
            "Evaluated episode 4 with a total return of -21.0\n",
            "This is episode number 4\n",
            "Evaluated episode 5 with a total return of -21.0\n",
            "Episode 175/200: -21.0\n",
            "Best performance so far! Saving model.\n",
            "Episode  176\n",
            "Episode  177\n",
            "Episode  178\n",
            "Episode  179\n",
            "Episode  180\n",
            "Episode  181\n",
            "Episode  182\n",
            "Episode  183\n",
            "Episode  184\n",
            "Episode  185\n",
            "Episode  186\n",
            "Episode  187\n",
            "Episode  188\n",
            "Episode  189\n",
            "Episode  190\n",
            "Episode  191\n",
            "Episode  192\n",
            "Episode  193\n",
            "Episode  194\n",
            "Episode  195\n",
            "Episode  196\n",
            "Episode  197\n",
            "Episode  198\n",
            "Episode  199\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluate.py\n",
        "\n",
        "def evaluate_policy(dqn, env, env_config, args, n_episodes, render=False, verbose=False):\n",
        "    \"\"\"Runs {n_episodes} episodes to evaluate current policy.\"\"\"\n",
        "    total_return = 0\n",
        "\n",
        "    for i in range(n_episodes):\n",
        "        #obs = preprocess(env.reset(), env=args[\"env\"]) #.unsqueeze(0)\n",
        "        obs_stack = preprocess(env.reset(), env=args[\"env\"]) #.unsqueeze(0)\n",
        "        done = False\n",
        "        episode_return = 0\n",
        "\n",
        "        while not done:\n",
        "            if render:\n",
        "                frame = env.render(mode=\"rgb_array\")\n",
        "                plt.imshow(frame)\n",
        "                plt.show() \n",
        "                display.clear_output(wait=True)\n",
        "            action = dqn.act(obs_stack, exploit=True)\n",
        "            #print(\"action: \", action)\n",
        "\n",
        "            obs, reward, done, info = env.step(action.item())\n",
        "            \n",
        "            if reward>0:\n",
        "                print(\"reward: \", reward)\n",
        "            #obs = preprocess(obs, env=args[\"env\"])#.unsqueeze(0)\n",
        "            if not done:\n",
        "                obs_stack = add_new_obs_to_stack(obs, obs_stack, env=args[\"env\"])\n",
        "            episode_return += reward\n",
        "        \n",
        "        total_return += episode_return\n",
        "        \n",
        "        if verbose:\n",
        "            print(f'Evaluated episode {i+1} with a total return of {episode_return}')\n",
        "\n",
        "    \n",
        "    return total_return / n_episodes"
      ],
      "metadata": {
        "id": "D8Q5tG6NnQeD"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "from gym import wrappers"
      ],
      "metadata": {
        "id": "oaCkGozSVZwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dqn = torch.load('Pong-v0_preval.pt', map_location=torch.device(device))\n",
        "env = gym.make('Pong-v0')\n",
        "env = gym.wrappers.AtariPreprocessing (env, screen_size=84 , grayscale_obs=True , frame_skip=1 , noop_max=30, scale_obs=True )\n",
        "env.reset()\n",
        "env_config = ENV_CONFIGS[\"Pong-v0\"]\n",
        "args = {\"evaluate_freq\": 25, \"evaluation_episodes\": 5, \"env\":'Pong-v0' }\n",
        "evaluate_policy(dqn, env, env_config, args, 1, render=True, verbose=True)"
      ],
      "metadata": {
        "id": "vB-0XbEo6T1i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "bd926c75-2ec8-41d3-dd78-cf888f30b593"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-76-c3dd519183c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0menv_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mENV_CONFIGS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Pong-v0\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"evaluate_freq\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"evaluation_episodes\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"env\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'Pong-v0'\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mevaluate_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdqn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-72-4a9b7f2520dc>\u001b[0m in \u001b[0;36mevaluate_policy\u001b[0;34m(dqn, env, env_config, args, n_episodes, render, verbose)\u001b[0m\n\u001b[1;32m     16\u001b[0m                 \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"rgb_array\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                 \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m                 \u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs_stack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexploit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    270\u001b[0m     \"\"\"\n\u001b[1;32m    271\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0m_show\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_show\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/pylab/backend_inline.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(close, block)\u001b[0m\n\u001b[1;32m     37\u001b[0m             display(\n\u001b[1;32m     38\u001b[0m                 \u001b[0mfigure_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                 \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_fetch_figure_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigure_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             )\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mdisplay\u001b[0;34m(*objs, **kwargs)\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0mpublish_display_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m             \u001b[0mformat_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mformat_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m                 \u001b[0;31m# nothing to display (e.g. _ipython_display_ took over)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36mformat\u001b[0;34m(self, obj, include, exclude)\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m                 \u001b[0;31m# FIXME: log the exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-2>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36mcatch_format_error\u001b[0;34m(method, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"show traceback on failed format call\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0;31m# don't warn on NotImplementedErrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    332\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mprinter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m             \u001b[0;31m# Finally look for special method names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(fig)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'png'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'retina'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'png2x'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mretina_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0mbytes_io\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_io\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytes_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfmt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'svg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m   2101\u001b[0m                     \u001b[0mbbox_artists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bbox_extra_artists\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2102\u001b[0m                     bbox_inches = self.figure.get_tightbbox(renderer,\n\u001b[0;32m-> 2103\u001b[0;31m                             bbox_extra_artists=bbox_artists)\n\u001b[0m\u001b[1;32m   2104\u001b[0m                     \u001b[0mpad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pad_inches\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2105\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mpad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36mget_tightbbox\u001b[0;34m(self, renderer, bbox_extra_artists)\u001b[0m\n\u001b[1;32m   2393\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2394\u001b[0m                     bbox = ax.get_tightbbox(renderer,\n\u001b[0;32m-> 2395\u001b[0;31m                             bbox_extra_artists=bbox_extra_artists)\n\u001b[0m\u001b[1;32m   2396\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2397\u001b[0m                     \u001b[0mbbox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tightbbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36mget_tightbbox\u001b[0;34m(self, renderer, call_axes_locator, bbox_extra_artists)\u001b[0m\n\u001b[1;32m   4325\u001b[0m                 \u001b[0mbb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbb_xaxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4327\u001b[0;31m             \u001b[0mbb_yaxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myaxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tightbbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4328\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbb_yaxis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4329\u001b[0m                 \u001b[0mbb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbb_yaxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36mget_tightbbox\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   1186\u001b[0m         \u001b[0mticks_to_draw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_ticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1188\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_label_position\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m         \u001b[0;31m# go back to just this axis's tick labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36m_update_label_position\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   2311\u001b[0m         \u001b[0;31m# get bounding boxes for this axis and any siblings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2312\u001b[0m         \u001b[0;31m# that have been set by `fig.align_ylabels()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2313\u001b[0;31m         \u001b[0mbboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbboxes2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tick_boxes_siblings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2315\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_position\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36m_get_tick_boxes_siblings\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   2296\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_siblings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2297\u001b[0m             \u001b[0mticks_to_draw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myaxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_ticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2298\u001b[0;31m             \u001b[0mtlb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtlb2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myaxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tick_bboxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mticks_to_draw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2299\u001b[0m             \u001b[0mbboxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtlb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2300\u001b[0m             \u001b[0mbboxes2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtlb2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36m_get_tick_bboxes\u001b[0;34m(self, ticks, renderer)\u001b[0m\n\u001b[1;32m   1172\u001b[0m         \u001b[0;34m\"\"\"Return lists of bboxes for ticks' label1's and label2's.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m         return ([tick.label1.get_window_extent(renderer)\n\u001b[0;32m-> 1174\u001b[0;31m                  for tick in ticks if tick.label1.get_visible()],\n\u001b[0m\u001b[1;32m   1175\u001b[0m                 [tick.label2.get_window_extent(renderer)\n\u001b[1;32m   1176\u001b[0m                  for tick in ticks if tick.label2.get_visible()])\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1172\u001b[0m         \u001b[0;34m\"\"\"Return lists of bboxes for ticks' label1's and label2's.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m         return ([tick.label1.get_window_extent(renderer)\n\u001b[0;32m-> 1174\u001b[0;31m                  for tick in ticks if tick.label1.get_visible()],\n\u001b[0m\u001b[1;32m   1175\u001b[0m                 [tick.label2.get_window_extent(renderer)\n\u001b[1;32m   1176\u001b[0m                  for tick in ticks if tick.label2.get_visible()])\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/text.py\u001b[0m in \u001b[0;36mget_window_extent\u001b[0;34m(self, renderer, dpi)\u001b[0m\n\u001b[1;32m    906\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_unitless_position\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 908\u001b[0;31m         \u001b[0mbbox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    909\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdpi\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdpi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdpi_orig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/transforms.py\u001b[0m in \u001b[0;36mtranslated\u001b[0;34m(self, tx, ty)\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtranslated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0;34m\"\"\"Construct a `Bbox` by translating this one by *tx* and *ty*.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mBbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_points\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcorners\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/transforms.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, points, **kwargs)\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mmeth\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mfrom_bounds\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mmeth\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mfrom_extents\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m         \"\"\"\n\u001b[0;32m--> 740\u001b[0;31m         \u001b[0mBboxBase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m         \u001b[0mpoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpoints\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "A8R5K3l02HSf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}